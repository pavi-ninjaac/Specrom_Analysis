{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#import the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.utils import resample\n\n#sklearn package \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn. preprocessing import LabelEncoder,StandardScaler\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import ComplementNB,MultinomialNB,GaussianNB \n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#model evaluation\nfrom sklearn.metrics import accuracy_score,classification_report, confusion_matrix,recall_score,precision_score,f1_score\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#read the dataset\ndata = pd.read_json('../input/news-category-dataset/News_Category_Dataset_v2.json',lines=True) # lines for avoid the trailing error\ncolumn = data.columns\ncolumn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['link','date'],axis=1,inplace = True)\ndata.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The shape of the dataset-------->\",data.shape)\nprint(\"The number of null values ------>\")\nprint(data.isnull().sum())\ncolumn = data.columns\nprint(\"The column present there-------->\",column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Category walkthrough"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The total number category present here------------->\",data['category'].nunique())\ncategory=data['category'].value_counts()\nprint(category)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,8))\nsns.barplot(x=category.index,y=category.values)\nplt.title(\"The distribution of categories\")\nplt.xlabel(\"Category\")\nplt.ylabel(\"The number of samples\")\n\nplt.xticks(rotation=60,fontsize = 14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pie chart \nplt.figure(figsize=(20,20))\nplt.pie(category.values, autopct=\"%1.1f%%\", labels=category.index)\nplt.show()\nplt.savefig(r\"./category_pie.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,13))\nsns.barplot(y=category.index,x=category.values)\nplt.title(\"The distribution of categories\")\nplt.xlabel(\"Category\")\nplt.ylabel(\"The number of samples\")\n\nplt.yticks(rotation=0,fontsize = 16)\nplt.show()\nplt.savefig(r\"./category_bar.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are unqual number of sample in each category, so we can drop some category and make it balanced"},{"metadata":{},"cell_type":"markdown","source":"# handling Dublicate and null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#delete the dublicate values\ndata.duplicated().sum() # count the total duplicate samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop_duplicates(keep='last',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#there can be dublicate of author names so check for the dublicate headline and short discription\ndata.duplicated(subset=['headline', 'short_description']).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop_duplicates(subset=['headline', 'short_description'],inplace=True,keep='last')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"THe length of the datset after dublicate deletion------>\",data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# there is no null value instead of null they are blank so we need to check for the blank placess and delete that\ndata[data['headline'] == '']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the blank values\nheadline_blank = data['headline'] == ''\ndata = data[~headline_blank]\nprint(\"THe length of the datset ------>\",data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop the blank short describtion column\ndescription_blank = data['short_description']==''\nprint(\"the lenth of the blank description samples----->\",len(data[description_blank]))\ndata = data[~description_blank]\nprint(\"THe length of the datset ---------------------->\",data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop the null author samples\nauthor_blank = data['authors']==''\nprint(\"the lenth of the blank auhtor samples---------->\",len(data[author_blank]))\ndata = data[~author_blank]\nprint(\"THe length of the datset ---------------------->\",data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#author \n#auhtor plot\nauhtor_count = data['authors'].value_counts()\n\nplt.figure(figsize=(25,18))\nsns.barplot(y=auhtor_count[:25].index,x=auhtor_count[:25].values)\nplt.title(\"The distribution of authors\")\nplt.xlabel(\"Author Name\")\nplt.ylabel(\"The number of samples\")\n\nplt.yticks(rotation=0,fontsize = 18)\nplt.show()\nplt.savefig(r\"./author_bar.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Balance the category data"},{"metadata":{"trusted":true},"cell_type":"code","source":"category = data['category'].value_counts()\nlist(category.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nwe can drop the Style, Education, College and Environment they are having very less number of sample, which may lead to less accuracy and f1 score."},{"metadata":{"trusted":true},"cell_type":"code","source":"cateo_keep = (data['category'] == 'POLITICS') | (data['category'] == 'WELLNESS' )| (data['category'] == 'ENTERTAINMENT') | (data['category'] == 'TRAVEL') | \\\n            (data['category'] == 'STYLE & BEAUTY') | (data['category'] == 'PARENTING' )| (data['category'] == 'HEALTHY LIVING') | (data['category'] == 'QUEER VOICES') | \\\n              (data['category'] == 'FOOD & DRINK') | (data['category'] == 'BUSINESS' )| (data['category'] == 'COMEDY') | (data['category'] == 'PARENTS') | (data['category'] == 'SPORTS') | (data['category'] == 'HOME & LIVING' )| (data['category'] == 'BLACK VOICES')\ndata = data[cateo_keep]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category = data['category'].value_counts()\ncategory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_1 = data[data['category'] == 'POLITICS']\ndata_1 = resample(data_1, replace=False, n_samples=3000, random_state=123)\ndata_2 = data[data['category'] == 'WELLNESS']\ndata_2 = resample(data_2, replace=False, n_samples=3000, random_state=123)\ndata_3 = data[data['category'] == 'ENTERTAINMENT']\ndata_3 = resample(data_3, replace=False, n_samples=3000, random_state=123)\ndata_4 = data[data['category'] == 'TRAVEL']\ndata_4 = resample(data_4, replace=False, n_samples=3000, random_state=123)\ndata_5 = data[data['category'] == 'STYLE & BEAUTY']\ndata_5 = resample(data_5, replace=False, n_samples=3000, random_state=123)\ndata_6 = data[data['category'] == 'PARENTING']\ndata_6 = resample(data_6, replace=False, n_samples=3000, random_state=123)\ndata_7 = data[data['category'] == 'HEALTHY LIVING']\ndata_7 = resample(data_7, replace=False, n_samples=3000, random_state=123)\ndata_8 = data[data['category'] == 'QUEER VOICES']\ndata_8 = resample(data_8, replace=False, n_samples=3000, random_state=123)\ndata_9 = data[data['category'] == 'FOOD & DRINK']\ndata_9 = resample(data_9, replace=False, n_samples=3000, random_state=123)\ndata_10 = data[data['category'] == 'BUSINESS']\ndata_10 = resample(data_10, replace=False, n_samples=3000, random_state=123)\ndata_11 = data[data['category'] == 'COMEDY']\ndata_11 = resample(data_11, replace=False, n_samples=3000, random_state=123)\ndata_12= data[data['category'] == 'PARENTS']\ndata_12 = resample(data_12, replace=False, n_samples=3000, random_state=123)\ndata_13= data[data['category'] == 'SPORTS']\ndata_13 = resample(data_13, replace=False, n_samples=3000, random_state=123)\ndata_14 = data[data['category'] == 'HOME & LIVING']\ndata_14 = resample(data_14, replace=False, n_samples=3000, random_state=123)\ndata_15 = data[data['category'] == 'BLACK VOICES']\ndata_15 = resample(data_15, replace=False, n_samples=3000, random_state=123)\n\ncato_list = [data_1 , data_2 , data_3 , data_4 ,data_5 , data_6 , data_7, data_8 , data_9 , data_10, data_11 , data_12 , data_13, data_14 ,data_15]\n\ndata = pd.concat(cato_list)\ndata['category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category = data['category'].value_counts()\nplt.figure(figsize=(25,13))\nsns.barplot(y=category.index,x=category.values)\nplt.title(\"The distribution of categories\")\nplt.xlabel(\"Category\")\nplt.ylabel(\"The number of samples\")\n\nplt.yticks(rotation=0,fontsize = 16)\nplt.show()\nplt.savefig(r\"./category_bar.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Column Combinning"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['text'] = data['headline']+'-'+data['short_description']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop the other columns\ndata.drop(['authors','headline','short_description'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The lenth of the datset-------------------->\",data.shape)\ndata.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\ndata = shuffle(data)\ndata.reset_index(inplace=True, drop=True) \ndata.head(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# test cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"corpus=[]\nfor i in range(100000):\n    text = data.iloc[i,1]\n    \n    text = text.lower()\n    text = re.sub('[^a-z0-9]',' ',text)\n    text = text.split()\n    \n    s = PorterStemmer()\n    text = [s.stem(word) for word in text if not word in set(stopwords.words('english')) ]\n    text = ' '.join(text)\n    corpus.append(text)\n    \n    if i%1000==0:\n        print(i,end='->')\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"corpus = pd.read_csv('../input/corpus/corpus.csv')\ncorpus\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# Tokazitation and Count Vectorization"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train and test split\nX = data['text']\n#label encoding the target\nlabel = LabelEncoder()\ny = label.fit_transform(data['category'])\n\n#split the train and test dataset\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.1,random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The X_train shape----->\",X_train.shape)\nprint('The X_text shape------>',X_test.shape)\nprint(\"THe y_train shape----->\",y_train.shape)\nprint(\"The y_test shape------>\",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ifidf vectorizer\n\nvecto =  TfidfVectorizer(stop_words='english',max_df = 0.99,min_df=10,\n                                   ngram_range=(1, 2),lowercase=True, max_features=5000)\nvecto = vecto.fit(X_train)\n\nX_train = vecto.transform(X_train).toarray()\nX_test = vecto.transform(X_test).toarray()\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(vecto.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_df = pd.DataFrame(X_train,columns = vecto.get_feature_names())\ntfidf_df.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The X_train shape----->\",X_train.shape)\nprint('The X_text shape------>',X_test.shape)\nprint(\"THe y_train shape----->\",y_train.shape)\nprint(\"The y_test shape------>\",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef classifier_scores(y_train,y_test, pred_train, pred_test):\n    \n    print()\n    print(\"Train data accuracy score: \", accuracy_score(y_train,pred_train))    \n    print(\"Test data accuracy score: \", accuracy_score(y_test,pred_test))\n    print()\n    print(\"Recall score on train data: \", recall_score(y_train,pred_train, average='macro'))\n    print(\"Recall score on test data: \",recall_score( y_test,pred_test, average='macro'))\n    print()\n    \n    print(\"Precision score on train data: \",precision_score(y_train,pred_train, average='macro'))\n    print(\"Precision score on test data: \",precision_score(y_test,pred_test, average='macro'))\n    print()\n    print(\"F1 score on train data: \",f1_score(y_train,pred_train, average='macro'))\n    print(\"F1 score on test data: \",f1_score(y_test,pred_test, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multinomial Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Multinamial NB----------------------------------->\")\nmultinb = MultinomialNB()\nmultinb.fit(X_train , y_train)\n\ny_train_pred = multinb.predict(X_train)\ny_test_pred = multinb.predict(X_test)\nclassifier_scores(y_train,y_test,y_train_pred,y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Complement Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Compiment NB----------------------------------->\")\ncompnb = ComplementNB(alpha=1.0)\ncompnb.fit(X_train , y_train)\n\ny_train_pred = compnb.predict(X_train)\ny_test_pred = compnb.predict(X_test)\nclassifier_scores(y_train,y_test,y_train_pred,y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gaussian Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"#model training\ngaussion_NB = GaussianNB()\ngaussion_NB.fit(X_train , y_train)\n\ny_train_pred = gaussion_NB.predict(X_train)\ny_test_pred = gaussion_NB.predict(X_test)\nclassifier_scores(y_train,y_test,y_train_pred,y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## logistic Regresssion"},{"metadata":{"trusted":true},"cell_type":"code","source":"#logistic Regresssion\n\nlog_reg = LogisticRegression()\n\nlog_reg.fit(X_train , y_train)\n\ny_train_pred = log_reg.predict(X_train)\ny_test_pred = log_reg.predict(X_test)\nclassifier_scores(y_train,y_test,y_train_pred,y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#logistric regression more accuracy\nlog_reg_hyper = LogisticRegression(solver='liblinear',n_jobs=-1,penalty='l2',)\nlog_reg_hyper.fit(X_train , y_train)\n\ny_train_pred = log_reg_hyper.predict(X_train)\ny_test_pred = log_reg_hyper.predict(X_test)\nclassifier_scores(y_train,y_test,y_train_pred,y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boosting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"#GradientBoostingClassifier \n\ngb_clf = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0)\ngb_clf.fit(X_train, y_train)\n\n\n    \ny_train_pred = gb_clf.predict(X_train)\ny_test_pred = gb_clf.predict(X_test)\nclassifier_scores(y_train,y_test,y_train_pred,y_test_pred)\nprint('-'*56)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model training with SVD"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_com = [500,1000,2000,3000]\ndef models_prepare():\n    model = {}\n    for n in n_com:\n        s = [('svd',TruncatedSVD(n_components = n)),('logistric',LogisticRegression())]\n        model[str(n)] = Pipeline(steps = s)\n    return model\nmodels = models_prepare()\nmodels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor name,model in models.items():\n    model.fit(X_train,y_train)\n    \n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    print(\"The Logistric Regression Trained with svd n_components {} \".format(name))\n    \n    classifier_scores(y_train,y_test,y_pred_train,y_pred_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# save the best model for later use"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n# save the model to disk\n\nprint(\"Loding your model..................\")\nfile_path = './logistricRegression_text_classi.sav'\npickle.dump(log_reg, open(file_path, 'wb'))\n \nprint('Model saved......')\n \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the model from disk\nloaded_model = pickle.load(open(file_path, 'rb'))\nresult = loaded_model.score(X_test, y_test)\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}