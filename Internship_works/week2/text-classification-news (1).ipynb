{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#import the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\n#sklearn package \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn. preprocessing import LabelEncoder,StandardScaler\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import ComplementNB,MultinomialNB,GaussianNB \n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#model evaluation\nfrom sklearn.metrics import accuracy_score,classification_report, confusion_matrix,recall_score,precision_score,f1_score\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#read the dataset\ndata = pd.read_json('../input/news-category-dataset/News_Category_Dataset_v2.json',lines=True) # lines for avoid the trailing error\ncolumn = data.columns\ncolumn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['link','date'],axis=1,inplace = True)\ndata.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The shape of the dataset-------->\",data.shape)\nprint(\"The number of null values ------>\")\nprint(data.isnull().sum())\ncolumn = data.columns\nprint(\"The column present there-------->\",column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Category walkthrough"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The total number category present here------------->\",data['category'].nunique())\ncategory=data['category'].value_counts()\nprint(category)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,8))\nsns.barplot(x=category.index,y=category.values)\nplt.title(\"The distribution of categories\")\nplt.xlabel(\"Category\")\nplt.ylabel(\"The number of samples\")\n\nplt.xticks(rotation=60,fontsize = 14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pie chart \nplt.figure(figsize=(20,20))\nplt.pie(category.values, autopct=\"%1.1f%%\", labels=category.index)\nplt.show()\nplt.savefig(r\"./category_pie.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = data['category'].value_counts().index\n\ndef groupper(grouplist,name):\n    for ele in categories:\n        if ele in grouplist:\n            data.loc[data['category'] == ele, 'category'] = name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"groupper( grouplist= ['SPORTS','ENTERTAINMENT' , 'COMEDY','WEIRD NEWS','ARTS'] , name =  'SPORTS AND ENTERTAINMENT')\n\ngroupper( grouplist= ['TRAVEL', 'ARTS & CULTURE','CULTURE & ARTS','FOOD & DRINK', 'TASTE'] , name =  'TRAVEL-TOURISM & ART-CULTURE')\n\ngroupper( grouplist= ['WOMEN','QUEER VOICES', 'LATINO VOICES', 'BLACK VOICES'] , name =  'EMPOWERED VOICES')\n\ngroupper( grouplist= ['BUSINESS' ,  'MONEY'] , name =  'BUSINESS-MONEY')\n\ngroupper( grouplist= ['THE WORLDPOST' , 'WORLDPOST' , 'WORLD NEWS'] , name =  'WORLDNEWS')\n\ngroupper( grouplist= ['ENVIRONMENT' ,'GREEN'] , name =  'ENVIRONMENT')\n\ngroupper( grouplist= ['TECH', 'SCIENCE'] , name =  'SCIENCE AND TECH')\n\ngroupper( grouplist= ['FIFTY' , 'IMPACT' ,'GOOD NEWS','CRIME'] , name =  'GENERAL')\n\ngroupper( grouplist= ['WEDDINGS', 'DIVORCE',  'RELIGION','MEDIA'] , name =  'MISC')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"We have a total of {} categories now\".format(data['category'].nunique()))\ncategory = data['category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pie chart \nplt.figure(figsize=(15,15))\nplt.pie(category.values, autopct=\"%1.1f%%\", labels=category.index)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,13))\nsns.barplot(y=category.index,x=category.values)\nplt.title(\"The distribution of categories\")\nplt.xlabel(\"Category\")\nplt.ylabel(\"The number of samples\")\n\nplt.yticks(rotation=0,fontsize = 16)\nplt.show()\nplt.savefig(r\"./category_bar.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are unqual number of sample in each category, so we can drop some category and make it balanced"},{"metadata":{},"cell_type":"markdown","source":"# handling Dublicate and null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#delete the dublicate values\ndata.duplicated().sum() # count the total duplicate samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop_duplicates(keep='last',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#there can be dublicate of author names so check for the dublicate headline and short discription\ndata.duplicated(subset=['headline', 'short_description']).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop_duplicates(subset=['headline', 'short_description'],inplace=True,keep='last')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"THe length of the datset after dublicate deletion------>\",data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# there is no null value instead of null they are blank so we need to check for the blank placess and delete that\ndata[data['headline'] == '']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the blank values\nheadline_blank = data['headline'] == ''\ndata = data[~headline_blank]\nprint(\"THe length of the datset ------>\",data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop the blank short describtion column\ndescription_blank = data['short_description']==''\nprint(\"the lenth of the blank description samples----->\",len(data[description_blank]))\ndata = data[~description_blank]\nprint(\"THe length of the datset ---------------------->\",data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop the null author samples\nauthor_blank = data['authors']==''\nprint(\"the lenth of the blank auhtor samples---------->\",len(data[author_blank]))\ndata = data[~author_blank]\nprint(\"THe length of the datset ---------------------->\",data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#author \n#auhtor plot\nauhtor_count = data['authors'].value_counts()\n\nplt.figure(figsize=(25,18))\nsns.barplot(y=auhtor_count[:25].index,x=auhtor_count[:25].values)\nplt.title(\"The distribution of authors\")\nplt.xlabel(\"Author Name\")\nplt.ylabel(\"The number of samples\")\n\nplt.yticks(rotation=0,fontsize = 18)\nplt.show()\nplt.savefig(r\"./author_bar.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Balance the category data"},{"metadata":{"trusted":true},"cell_type":"code","source":"category = data['category'].value_counts()\ncategory","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nwe can drop the Style, Education, College and Environment they are having very less number of sample, which may lead to less accuracy and f1 score."},{"metadata":{"trusted":true},"cell_type":"code","source":"cateo_drop = (data['category'] == 'ENVIRONMENT') | (data['category'] == 'STYLE' )| (data['category'] == 'EDUCATION') | (data['category'] == 'COLLEGE')\ndata = data[~cateo_drop]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.copy()\ndata = data.groupby('category').head(3000)\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category = data['category'].value_counts()\nplt.figure(figsize=(25,13))\nsns.barplot(y=category.index,x=category.values)\nplt.title(\"The distribution of categories\")\nplt.xlabel(\"Category\")\nplt.ylabel(\"The number of samples\")\n\nplt.yticks(rotation=0,fontsize = 16)\nplt.show()\nplt.savefig(r\"./category_bar.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Column Combinning"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['text'] = data['headline']+'-'+data['short_description']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop the other columns\ndata.drop(['authors','headline','short_description'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The lenth of the datset-------------------->\",data.shape)\ndata.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\ndata = shuffle(data)\ndata.reset_index(inplace=True, drop=True) \ndata.head(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# test cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"corpus=[]\nfor i in range(100000):\n    text = data.iloc[i,1]\n    \n    text = text.lower()\n    text = re.sub('[^a-z0-9]',' ',text)\n    text = text.split()\n    \n    s = PorterStemmer()\n    text = [s.stem(word) for word in text if not word in set(stopwords.words('english')) ]\n    text = ' '.join(text)\n    corpus.append(text)\n    \n    if i%1000==0:\n        print(i,end='->')\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"corpus = pd.read_csv('../input/corpus/corpus.csv')\ncorpus\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# Tokazitation and Count Vectorization"},{"metadata":{"trusted":true},"cell_type":"code","source":"#ifidf vectorizer\nX = data['text']\n\nvecto =  TfidfVectorizer(stop_words='english',max_df = 0.99,min_df=0.001,\n                                   ngram_range=(1, 2),lowercase=True, max_features=5000)\nX = vecto.fit_transform(X).toarray()\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(vecto.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_df = pd.DataFrame(X,columns = vecto.get_feature_names())\ntfidf_df.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#label encoding the target\nlabel = LabelEncoder()\ny = label.fit_transform(data['category'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train and test split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.1,random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The X_train shape----->\",X_train.shape)\nprint('The X_text shape------>',X_test.shape)\nprint(\"THe y_train shape----->\",y_train.shape)\nprint(\"The y_test shape------>\",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef classifier_scores(y_train,y_test, pred_train, pred_test):\n    \n    print()\n    print(\"Train data accuracy score: \", accuracy_score(y_train,pred_train))    \n    print(\"Test data accuracy score: \", accuracy_score(y_test,pred_test))\n    print()\n    print(\"Recall score on train data: \", recall_score(y_train,pred_train, average='macro'))\n    print(\"Recall score on test data: \",recall_score( y_test,pred_test, average='macro'))\n    print()\n    \n    print(\"Precision score on train data: \",precision_score(y_train,pred_train, average='macro'))\n    print(\"Precision score on test data: \",precision_score(y_test,pred_test, average='macro'))\n    print()\n    print(\"F1 score on train data: \",f1_score(y_train,pred_train, average='macro'))\n    print(\"F1 score on test data: \",f1_score(y_test,pred_test, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multinomial Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Multinamial NB----------------------------------->\")\nmultinb = MultinomialNB()\nmultinb.fit(X_train , y_train)\n\ny_train_pred = multinb.predict(X_train)\ny_test_pred = multinb.predict(X_test)\nclassifier_scores(y_train,y_test,y_train_pred,y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Complement Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Compiment NB----------------------------------->\")\ncompnb = ComplementNB(alpha=1.0)\ncompnb.fit(X_train , y_train)\n\ny_train_pred = compnb.predict(X_train)\ny_test_pred = compnb.predict(X_test)\nclassifier_scores(y_train,y_test,y_train_pred,y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gaussian Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"#model training\ngaussion_NB = GaussianNB()\ngaussion_NB.fit(X_train , y_train)\n\ny_train_pred = gaussion_NB.predict(X_train)\ny_test_pred = gaussion_NB.predict(X_test)\nclassifier_scores(y_train,y_test,y_train_pred,y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## logistic Regresssion"},{"metadata":{"trusted":true},"cell_type":"code","source":"#logistic Regresssion\n\nlog_reg = LogisticRegression()\n\nlog_reg.fit(X_train , y_train)\n\ny_train_pred = log_reg.predict(X_train)\ny_test_pred = log_reg.predict(X_test)\nclassifier_scores(y_train,y_test,y_train_pred,y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#logistric regression more accuracy\nlog_reg_hyper = LogisticRegression(solver='liblinear',n_jobs=-1,penalty='l2',)\nlog_reg_hyper.fit(X_train , y_train)\n\ny_train_pred = log_reg_hyper.predict(X_train)\ny_test_pred = log_reg_hyper.predict(X_test)\nclassifier_scores(y_train,y_test,y_train_pred,y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boosting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"#GradientBoostingClassifier \nlr_list = [0.05, 0.075, 0.1, 0.25]\n\nfor learning_rate in lr_list:\n    gb_clf = GradientBoostingClassifier(n_estimators=20, learning_rate=learning_rate, max_features=2, max_depth=2, random_state=0)\n    gb_clf.fit(X_train, y_train)\n\n    print(\"Learning rate: \", learning_rate)\n    \n    y_train_pred = gb_clf.predict(X_train)\n    y_test_pred = gb_clf.predict(X_test)\n    classifier_scores(y_train,y_test,y_train_pred,y_test_pred)\n    print('-'*56)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model training with SVD"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_com = [500,700,1000,1500]\ndef models_prepare():\n    model = {}\n    for n in n_com:\n        s = [('svd',TruncatedSVD(n_components = n)),('logistric',LogisticRegression())]\n        model[str(n)] = Pipeline(steps = s)\n    return model\nmodels = models_prepare()\nmodels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor name,model in models.items():\n    model.fit(X_train,y_train)\n    \n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    print(\"The Logistric Regression Trained with svd n_components {} \".format(name))\n    \n    classifier_scores(y_train,y_test,y_pred_train,y_pred_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## logistic regresion"},{"metadata":{"trusted":true},"cell_type":"code","source":"log = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\n\n\n# define grid search\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\ngrid_search = GridSearchCV(estimator=log, param_grid=grid, n_jobs=-1, cv=5, scoring='accuracy',error_score=0)\n\n\ngrid_result = grid_search.fit(X, y)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\n\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f : %r\" % (mean, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}